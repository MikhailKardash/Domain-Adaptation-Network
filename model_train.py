# -*- coding: utf-8 -*-
"""DA_NN_New

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C5434jMFP-i2IdDcd253GpXD4JWrko8b
"""

# mycode start
import torch
#import torchvision
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
#from sklearn import naive_bayes
#from sklearn.neighbors import KNeighborsClassifier
from Libraries.theLoss import theLoss
from Libraries.DANet2 import DANet2


#data paths
X_source = torch.load("./data_use/Source_dat.pt")
X_target = torch.load("./data_use/Target_dat.pt")

print(X_source.shape)
print(X_target.shape)

#labels path
labels_tot = torch.load("./data_use/labels.pt")
labels_target = torch.load("./data_use/t_labels.pt")

loss_store = list()

#play around with hyperparameters, remember it's training on CPU. 
batch_size = 500
#make batch size
data_len = X_source.shape[0]
steps_per_epoch = int(np.floor(data_len / batch_size))


#specify NN dimensions
#usedim = 256 #specify input tensor dimension if need be
#dims = [usedim,usedim,int(np.floor(usedim * .8)),int(np.floor(usedim * .6))]
#dims = [128,180,210,256]
dims = [7,14,28,56]

DA_nn = DANet([dims[0],dims[1],dims[2], dims[2], dims[2], dims[3],dims[3]], torch.tanh)


#make loss hyperparameters
criterion = theLoss(alpha=.8, beta=10, gamma=0.01, omega=(1,.2,.2,.2,0), tau=(0,0,0,0,0), k1=25, k2=35)

#set learning rate
#lr = 1
lr = .1
#lr = 1e-2

num_epochs = 50

for epoch in range(0, num_epochs):
    for k in range(0, int(steps_per_epoch)):
        # preprocessing
        it = batch_size * k
        X_tens = X_source[it : it + batch_size]
        labels = labels_tot[it : it + batch_size]
        
        inds = np.random.permutation(X_target.shape[0])
        labels2 = labels_target[inds[:batch_size]]
        X_unlabl = X_target[inds[:batch_size*2]]
        
        #x_dat = X_tens
        x_dat = torch.cat((X_tens,X_unlabl[:batch_size]),0)
        labels = torch.cat((labels,labels2),0)
        labels = labels.data.numpy()
        x_dat_u = X_unlabl
        
        #run labeled data
        zs,h_tanh_s = DA_nn.forward(x_dat)

        # run unlabeled data
        zt,h_tanh_t = DA_nn.forward(x_dat_u)

        # run loss

        loss, j = criterion.forward(Model=DA_nn, xlabl=X_tens, xun=X_unlabl, actS = h_tanh_s, actT = h_tanh_t,
                                             labels=labels)
        print("Epoch: " + str(epoch) + " Iter: " + str(k))
        print("Loss: " + str(float(loss)))
        loss_store.append(float(loss))
        #if (float(loss) < 0.1):
        #    break
        # backpropagate

        grad_out,bias_out = criterion.backward(Model = DA_nn, xlabl = x_dat,
                                               xun = X_unlabl, actS = h_tanh_s,
                                               actT = h_tanh_t, zS = zs, zT = zt,
                                               j = j, labels = labels)
        with torch.no_grad():
            # update weights + biases of each layer
            for i in range(DA_nn.numLayers):
                W = torch.Tensor(DA_nn.linears[i].weight)
                W = W - lr*grad_out[i]
                DA_nn.linears[i].weight.copy_(W)
                B = torch.Tensor(DA_nn.linears[i].bias)
                B = B - lr*bias_out[i]
                DA_nn.linears[i].bias.copy_(B)

        #lr = lr * .98
        #lr = lr * 1.01
        print("LR: " + str(lr))
        #if (float(loss) < 0.1):

with open('./data_use/loss2.txt', 'w') as f:
    for item in loss_store:
        f.write("%f\n" % item)

torch.save(DA_nn, './data_use/model.pt')